{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06bd18b-e761-4061-bcbe-d2e04a11d43a",
   "metadata": {},
   "source": [
    "# Customised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04f00408-1d11-4eb4-b6ba-5328c39eeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_chain(model, retriever, rag_prompt = None):\n",
    "    # We will use a prompt template from langchain hub.\n",
    "    if not rag_prompt:\n",
    "        rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # And we will use the LangChain RunnablePassthrough to add some custom processing into our chain.\n",
    "    rag_chain = (\n",
    "            {\n",
    "                \"context\": RunnableLambda(get_question) | retriever | format_docs,\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | rag_prompt\n",
    "            | model\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "def get_question(input):\n",
    "    if not input:\n",
    "        return None\n",
    "    elif isinstance(input,str):\n",
    "        return input\n",
    "    elif isinstance(input,dict) and 'question' in input:\n",
    "        return input['question']\n",
    "    elif isinstance(input,BaseMessage):\n",
    "        return input.content\n",
    "    else:\n",
    "        raise Exception(\"string or dict with 'question' key expected as RAG chain input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684dcdab-1690-41e4-a864-a756df791545",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "/home/fish/anaconda3/envs/jusjus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 635, which is longer than the specified 500\n",
      "Created a chunk of size 868, which is longer than the specified 500\n",
      "Created a chunk of size 606, which is longer than the specified 500\n",
      "Created a chunk of size 1257, which is longer than the specified 500\n",
      "Created a chunk of size 533, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/fish/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "\n",
    "# Set up the Hugging Face Hub API token\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Define the repository ID for the Gemma 2b model\n",
    "repo_id = \"google/gemma-2b-it\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, max_length=1024, temperature=1.5\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"/home/fish/Documents/optymize.xlsx\")\n",
    "data_list = df.values.ravel().tolist()\n",
    "document_list = []\n",
    "\n",
    "for content in data_list:\n",
    "    document = Document(content=content, page_content=content)\n",
    "    document_list.append(document)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(document_list)\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(docs, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e962be5-9aa6-4e0e-b926-c9ff3243fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 20})\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8bae9bc-6515-46d6-bf1e-222e081039d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "rag_chain = make_rag_chain(llm, retriever) | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a4cde1-a7ba-4af0-86fc-33e11264d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- QUESTION:  what is Optymize?\n",
      "* Ans:\n",
      "  Optymize allows users to stake their tokens to earn return in the form of OPZ (Optimizable Z tokens) plus an opportunity to earn common ownership rewards via gOPZ. This means the number of Optymize Vaults that can be created is limited by capital and user stake rates to ensure the network remains sustainable.\n",
      "\n",
      "--- QUESTION:  how can i deposite coin on Optymize?\n",
      "* Ans:\n",
      "  Deposit by staking your tokens into the Optymize Vault, bearing the risk of a Security Incident. Each token can be deposited for the purpose of earning both Opz revenues and tiered APY rewards related to liquidity pool activities on the exchange.\n",
      "\n",
      "--- QUESTION:  what is Optymize's twitter?\n",
      "* Ans:\n",
      "  Optymize's Twitter handle is @Optymize_xyz.\n",
      "\n",
      "--- QUESTION:  what is gOPZ tokens?\n",
      "* Ans:\n",
      "  The gOPZ tokens can be earned by staking OPZ tokens with us. The length of lockup depends on the user’s preference, but it generally starts from 60 days and can go as far as 24 months.\n",
      "\n",
      "--- QUESTION:  what is Optymize tokenomics?\n",
      "* Ans:\n",
      "  Optymize tokenomics involves selecting tokens to be pooled together based on various factors like TVL, risk, audit scores, market demand, and community feedback. The tokens can be staked to earn APY, and from there investors can indirectly contribute to the profitability of the protocol.\n",
      "\n",
      "--- QUESTION:  what is Optymize details tokenomics?\n",
      "* Ans:\n",
      "  Optymize details tokenomics are the factors that contribute to the creation and management of stable coins within the Optymize Vault by selectively pooling tokens based on several factors including TVL, risk ranking, audit scores, market demand and community feedback.\n",
      "\n",
      "--- QUESTION:  Optymize Vault Model – How does it works?, detail explaination\n",
      "* Ans:\n",
      "  Optymize Vault Model creates vaults for the staking of tokens to earn yield and mitigate Security Incident risk. It shares losses proportionally among all depositors when a Security Incident happens, and uses an intelligent algorithm to manage risks while sharing token prices proportionally among all depositors.\n",
      "\n",
      "--- QUESTION:  Optymize Vault Model – How does it works?\n",
      "* Ans:\n",
      "  The Optymize Vault model utilizes a risk-sharing system among all the depositors by pooling their tokens and sharing the losses proportionally during Security Incidents.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "        \"what is Optymize?\",\n",
    "        \"how can i deposite coin on Optymize?\",\n",
    "        \"what is Optymize's twitter?\",\n",
    "        \"what is gOPZ tokens?\",\n",
    "        \"what is Optymize tokenomics?\",\n",
    "        \"what is Optymize details tokenomics?\",\n",
    "        \"Optymize Vault Model – How does it works?, detail explaination\",\n",
    "        \"Optymize Vault Model – How does it works?\"\n",
    "        ]\n",
    "for q in questions:\n",
    "    print(\"\\n--- QUESTION: \", q)\n",
    "    print(\"* Ans:\\n\", rag_chain.invoke(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff1ce4f1-20a0-4318-9534-701042f40627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'The Optymize Protocol is a first-of-its-kind multi-blockchain solution that combines both yield enhancement and risk mitigation for crypto assets.\\nExplain more on the keywords here',\n",
       " 'text': '\\n\\n- **Multi-blockchain**: It highlights that the protocol operates across multiple blockchains, opening up opportunities for broader reach and cross-asset market participation.\\n\\n- **Yield Enhancement**: This means finding and securing opportunities to earn additional income from crypto assets, often through staking or lending processes.\\n\\n- **Risk Mitigation**: It suggests strategies and measures taken to minimize potential financial losses and protect capital against market volatility.\\n\\n- ** crypto assets**: This refers to various cryptocurrencies and other crypto-based assets, such as NFTs or DeFi protocols.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"The Optymize Protocol is a first-of-its-kind multi-blockchain solution that combines both yield enhancement and risk mitigation for crypto assets.\n",
    "Explain more on the keywords here\"\"\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "011afd35-5e4a-4b1a-a8a3-8b48e8451f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\n- **Multi-blockchain**: It highlights that the protocol operates across multiple blockchains, opening up opportunities for broader reach and cross-asset market participation.\\n\\n- **Yield Enhancement**: This means finding and securing opportunities to earn additional income from crypto assets, often through staking or lending processes.\\n\\n- **Risk Mitigation**: It suggests strategies and measures taken to minimize potential financial losses and protect capital against market volatility.\\n\\n- ** crypto assets**: This refers to various cryptocurrencies and other crypto-based assets, such as NFTs or DeFi protocols.')]]\n"
     ]
    }
   ],
   "source": [
    "qs = [{'question': question}]\n",
    "res = llm_chain.generate(qs)\n",
    "print(res.generations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jusjus",
   "language": "python",
   "name": "jusjus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
